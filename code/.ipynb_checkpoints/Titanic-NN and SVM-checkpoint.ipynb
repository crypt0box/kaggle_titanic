{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from sklearn import svm\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.optimizers import RMSprop\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Going to use these 5 base models for the stacking\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier,\n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Load in the train and test datasets\n",
    "train = pd.read_csv('C:/Users/tenki/Desktop/kaggleデータセット/titanic/train.csv')\n",
    "test = pd.read_csv('C:/Users/tenki/Desktop/kaggleデータセット/titanic/test.csv')\n",
    "data=train\n",
    "# Store our passenger ID for easy access\n",
    "PassengerId = test['PassengerId']\n",
    "\n",
    "# print(train.head(3))\n",
    "\n",
    "# data\n",
    "full_data = [train, test]\n",
    "\n",
    "# Some features of my own that I have added in\n",
    "# Gives the length of the name\n",
    "train['Name_length'] = train['Name'].apply(len)\n",
    "test['Name_length'] = test['Name'].apply(len)\n",
    "# Feature that tells whether a passenger had a cabin on the Titanic\n",
    "train['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
    "test['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
    "\n",
    "# Feature engineering steps taken from Sina\n",
    "# Create new feature FamilySize as a combination of SibSp and Parch\n",
    "for dataset in full_data:\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "# Create new feature IsAlone from FamilySize\n",
    "for dataset in full_data:\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "# Remove all NULLS in the Embarked column\n",
    "for dataset in full_data:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n",
    "# Remove all NULLS in the Fare column and create a new feature CategoricalFare\n",
    "for dataset in full_data:\n",
    "    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n",
    "train['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n",
    "# Create a New feature CategoricalAge\n",
    "for dataset in full_data:\n",
    "    age_avg = dataset['Age'].mean()\n",
    "    age_std = dataset['Age'].std()\n",
    "    age_null_count = dataset['Age'].isnull().sum()\n",
    "    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n",
    "    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n",
    "    dataset['Age'] = dataset['Age'].astype(int)\n",
    "train['CategoricalAge'] = pd.cut(train['Age'], 5)\n",
    "\n",
    "\n",
    "# Define function to extract titles from passenger names\n",
    "def get_title(name):\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "# Create a new feature Title, containing the titles of passenger names\n",
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Name'].apply(get_title)\n",
    "# Group all non-common titles into one single grouping \"Rare\"\n",
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Title'].replace(\n",
    "        ['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "for dataset in full_data:\n",
    "    # Mapping Sex\n",
    "    dataset['Sex'] = dataset['Sex'].map({'female': 0, 'male': 1}).astype(int)\n",
    "\n",
    "    # Mapping titles\n",
    "    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "    dataset['Title'] = dataset['Title'].map(title_mapping)\n",
    "    dataset['Title'] = dataset['Title'].fillna(0)\n",
    "\n",
    "    # Mapping Embarked\n",
    "    dataset['Embarked'] = dataset['Embarked'].map({'S': 0, 'C': 1, 'Q': 2}).astype(int)\n",
    "\n",
    "    # Mapping Fare\n",
    "    dataset.loc[dataset['Fare'] <= 7.91, 'Fare'] = 0\n",
    "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
    "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare'] = 2\n",
    "    dataset.loc[dataset['Fare'] > 31, 'Fare'] = 3\n",
    "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
    "\n",
    "    # Mapping Age\n",
    "    dataset.loc[dataset['Age'] <= 16, 'Age'] = 0\n",
    "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
    "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
    "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
    "    dataset.loc[dataset['Age'] > 64, 'Age'] = 4;\n",
    "\n",
    "\n",
    "# Feature selection\n",
    "drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\n",
    "train = train.drop(drop_elements, axis = 1)\n",
    "train = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\n",
    "test  = test.drop(drop_elements, axis = 1)\n",
    "# print(train.head(3))\n",
    "\n",
    "colormap = plt.cm.RdBu\n",
    "plt.figure(figsize=(14,12))\n",
    "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0,square=True, cmap=colormap, linecolor='white', annot=True)\n",
    "# plt.show()\n",
    "\n",
    "g = sns.pairplot(train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked',\n",
    "       u'FamilySize', u'Title']], hue='Survived', palette = 'seismic',size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\n",
    "g.set(xticklabels=[])\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\n",
    "y_train = train['Survived'].ravel()\n",
    "train = train.drop(['Survived'], axis=1)\n",
    "x_train = train.values # Creates an array of the train data\n",
    "x_test = test.values # Creats an array of the test data\n",
    "\n",
    "x_train_tr=x_train[0:601]\n",
    "x_train_te=x_train[601:891]\n",
    "y_train_tr=y_train[0:601]\n",
    "y_train_te=y_train[601:891]\n",
    "\n",
    "# ニューラルネットワーク構築\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim = 11, activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, nb_epoch=1000,batch_size=100, validation_split=0.2)\n",
    "pred_tr=model.predict(x_train_tr)\n",
    "pred_te=model.predict(x_train_te)\n",
    "pred_test=model.predict(x_test)\n",
    "[loss,accuracy_score] = model.evaluate(x_train_te, y_train_te)\n",
    "print(\"loss:{0} -- accuracy:{1}\".format(loss,accuracy_score))\n",
    "\n",
    "p = [1 if i > 0.5 else 0 for i in pred_te]\n",
    "print('Accuracy at NN:n',sum(p == y_train_te) / 290.0)\n",
    "\n",
    "# svm\n",
    "classifier = svm.SVC(C=1.0, gamma=0.01)\n",
    "classifier.fit(pred_tr, y_train_tr)\n",
    "predicted = classifier.predict(pred_te)\n",
    "print('Accuracy at NN+svm1:n', metrics.accuracy_score(y_train_te, predicted))\n",
    "\n",
    "param_grid = [\n",
    "    {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "    {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}]\n",
    "\n",
    "clf = GridSearchCV(SVC(C=1), param_grid, cv=5, n_jobs=-1)  # n_jobs: 並列計算を行う（-1 とすれば使用PCで可能な最適数の並列処理を行う）\n",
    "clf.fit(x_train, y_train)\n",
    "# print(clf.best_estimator_)  # 最適なパラメータを表示\n",
    "\n",
    "pred=clf.predict(x_test)\n",
    "# print(classification_report(y_train_te, pred))  # クラスタリング結果を表示\n",
    "# print(confusion_matrix(y_train_te, pred))  # クラスタリング結果を表示\n",
    "# print('Accuracy at NN+svm2:n', metrics.accuracy_score(y_train_te, pred))\n",
    "\n",
    "# output_df\n",
    "test_pred=model.predict(x_test)\n",
    "test_pred = [1 if i > 0.5 else 0 for i in test_pred]\n",
    "\n",
    "output_df = pd.DataFrame(test_pred, columns=['Survived'])\n",
    "output_df['PassengerId'] = PassengerId\n",
    "output_df = output_df.ix[:, ['PassengerId', 'Survived']]\n",
    "output_df.to_csv('C:/Users/tenki/Desktop/kaggleデータセット/titanic/test_ans_nn.csv', index=False, encoding='utf-8')\n",
    "\n",
    "output_df = pd.DataFrame(pred, columns=['Survived'])\n",
    "output_df['PassengerId'] = PassengerId\n",
    "output_df = output_df.ix[:, ['PassengerId', 'Survived']]\n",
    "output_df.to_csv('C:/Users/tenki/Desktop/kaggleデータセット/titanic/test_ans_svm.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
